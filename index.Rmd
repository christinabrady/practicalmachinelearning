---
title: 'Practical Machine Learning: Motion Analysis'
author: "Christina Brady"
date: "June 3, 2016"
output: html_document
---
```{r, include = FALSE, cache= FALSE}
source("./report_code.R")
library(caret)
```
# 1. Executive Summary

The objective of this analysis is to use data collected from human recognition activity monitors on the waist, wrist and bicep of 6 participantsplus one sensor on the dumbbell as they performed dumbbell curls in five different manners to classify in which manner the dumbbell curl is performed. They each performed the dumbbell curl "correctly" (as recommended) and they each performed the dumbbell curls making 4 common mistakes. The data is licensed under the Creative Commons license (CC BY-SA) and can be found here [link](http://groupware.les.inf.puc-rio.br/har). The nature of the problem being one of classification with relatively sparse data (the participants only performed 10 repetitions in five different manners), I chose to use a random forest both to aid in feature selection and as the predictive model. 

# 2. Data
I downloaded the data from the class website in csv format and read it into R. The initial dataset contains 19622 observations on 159 variables. The first six variables include the name of the participant, various time stamps and window classifications used by the authors of the study. Many of the other variables are composite or calculated variables such as minimun, maximum, mean or totals. Of the variables that are some form of data from the sensors, some variables imported as character, others imported as numeric and finally others imported as integer variables. By converting them all to numeric variables, I was able to determine that many of the variables have very sparse data.

# 3. Feature Selection
As mentioned above, there are three types of time stamp variables. The raw_time_stamp_part1 is a numeric variable that starts at 1322489605 and increments by 1 until it reaches 1323095081. There is no discernable pattern in the frequency of the values. Similarly, there is no discernable pattern between raw_timestamp_part1 and raw_timestamp_part2. Finally, a frequency table of user name, cvtd timestamp and classe shows that this timestamp is probably incomplete. A number of timestamps have no data and each user's data occurs exclusively 2-3 minute windows, often with different "performances" occurring during the same time stamp.

```{r}
table(pml$cvtd_timestamp, pml$classe, pml$user_name)[, , 1]
```

Thus, I decided to eliminate the timestamps. Furthermore, each person appears to have completed the exercises at different times. As a result, using the username and or the cvtd_timestamp would probably lead to overfitting.

Next, using the following code, I determined which columns contain a significant number of missing values and eliminated those variable from the analysis. I also eliminated the rows where the new_window value was "yes" because that also correlated highly with missing values. Finally, I created  function that takes a data frame as an input and applies all of the cleaning steps so that it is easier to ensure that I am performing the same tranformations on both the training and cross validation set. 

```{r, eval = FALSE}
natest <- function(c){
  return(as.character(summary(c)[7]))
}

createNAcolmap <- function(dat){
  for(i in 6:(ncol(dat)-1)){
    dat[, i] <- as.numeric(dat[, i])
  }
  nacol <- list()
  for(i in 1:ncol(dat)){
    nacol[[i]] <- natest(dat[, i])
  }
  map <- unlist(nacol)
  names(map) <- colnames(dat)
  return(map[!is.na(map)])  
}

nacolmap <- createNAcolmap(training)
elim_name <- c("raw_timestamp_part_2", "cvtd_timestamp", "num_window", "raw_timestamp_part_1", "user_name") 


clean <- function(dat){
  nonas <- dat[,!(colnames(dat) %in% names(nacolmap))]
  return(nonas[, !(colnames(nonas) %in% elim_name)])
}
```

# 4. Building the Model
I used the caret packages to split the data and train the model. Since the course provides a test set, I split the data as if I had a training set (80%), a cross validation set (20%) and a test set (unknown). 

```{r, eval = FALSE}
inTrain <- createDataPartition(y = pml$classe, p = .8, list = FALSE)
training <- pml[inTrain,]
testing <- pml[-inTrain,]

cleantraining <- clean(training)
cleantraining$classe <- factor(cleantraining$factor)  ### random forest expects the dependent variable to be a factor
cleantesting <- clean(testing)   

```

I tested 1. a random forest with 100 trees on the clean data, 2. a random forest on the clean data with cross validation and 100 trees, and 3. a random forest using variables selected through principle component analysis (with 100 trees). 
```{r}
set.seed(1979)

## model 1
rf1 <- randomForest(classe~ ., data = cleantraining, ntree = 100) ##1.5 minues
# rf1 <- train(classe~ ., method = "rf", data = cleantraining, ntree = 100)
confusionMatrix(predict(rf1, newdata = cleantesting[, 1:53]), cleantesting$classe)

## model 2
pp <- preProcess(cleantraining, method = "pca")
trainingpp <- predict(pp, newdata = cleantraining)
rf2 <- randomForest(classe~., data = trainingpp, ntree = 100)
# rf2 <- train(classe ~., method ="rf", data = trainingpp, ntree = 100)
testpp <- predict(pp, newdata = cleantesting[, 1:53])
confusionMatrix(predict(rf2, newdata = testpp), cleantesting$classe) 

## model 3
rf3 <- train(classe~., method = "rf", data = cleantraining, trControl = trainControl(method = "repeatedcv", number = 20), ntree=100)
confusionMatrix(predict(rf3, cleantesting[, 1:53]), cleantesting$classe)
```
I used the random forest package in the first 2 models because it is considerably faster than the train function of the caret package.

# 5. Results
The models 1 and 3 returned an accuracy of 0.99 on my validation set. Both models erred in very similar ways. The same accuracy and very similar errors made me suspect that were overfitting. I confirmed my suspicion when I ran used the model to predict the test data and submitted it to Coursera. The last model performed the best. The accuracy of this model on this test set is 0.98 on my validation set and 0.95 on the test set.

# References:
1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.


